{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuZVPb7AEDeJGtXqUNHs1b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherif17/Hand_Signature_Detection_Verification/blob/main/Hand_Signature_detection_Yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ],
      "metadata": {
        "id": "3Lfr8XQ17AEm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxJtUwRA2Q52"
      },
      "outputs": [],
      "source": [
        "import os, time, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2, torch\n",
        "import os\n",
        "import csv\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import shutil as sh\n",
        "from sklearn import preprocessing, model_selection\n",
        "\n",
        "from IPython.display import Image, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UK1Vf-S29Ci",
        "outputId": "3fafd087-1538-44ad-f7bb-b4d016c54e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yudO3-wi7SnG",
        "outputId": "3ab4ec43-f44a-43d6-940c-e0db5dcada97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 15529, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 15529 (delta 49), reused 118 (delta 43), pack-reused 15393\u001b[K\n",
            "Receiving objects: 100% (15529/15529), 14.59 MiB | 17.17 MiB/s, done.\n",
            "Resolving deltas: 100% (10573/10573), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install GitPython\n",
        "!wget https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "!gdown https://drive.google.com/u/0/uc?id=1gew1zSfSZKiUKGPGc3bpGXbO03HvE9-_\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1v79mdeZuSY_P7NfK8oNNM0n7qsRlaEG_/view"
      ],
      "metadata": {
        "id": "X0jentRo8tx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reading DataSet**"
      ],
      "metadata": {
        "id": "2fbcKvLf7hl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the dataset folder\n",
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/AI-ITI/Computer_vision/Assignment_Three_Detection/SignatureObjectDetection'"
      ],
      "metadata": {
        "id": "Eg0ROTZn23ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Annotating DataSet**"
      ],
      "metadata": {
        "id": "9T6h448P9SZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_annotate(folder_path, csv_file, img_path):\n",
        "\n",
        "    # List to store the annotations\n",
        "    annotations = []\n",
        "    # Iterate over the text files in the folder\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.txt'):\n",
        "            with open(os.path.join(folder_path, file_name), 'r') as f:\n",
        "                # Read all the lines in the file\n",
        "                lines = f.readlines()\n",
        "                # Process each line\n",
        "                for line in lines:\n",
        "                    # Extract the bbox coordinates from the line\n",
        "                    bbox = line.strip().split(',')\n",
        "                    # Add the annotation to the list\n",
        "                    xmin, ymin, xmax, ymax = list(map(int,[bbox[0], bbox[1], bbox[2], bbox[3]]))\n",
        "                    xcenter = (xmin+xmax)/2\n",
        "                    ycenter = (ymin+ymax)/2\n",
        "                    w = xmax-xmin\n",
        "                    h = ymax-ymin\n",
        "\n",
        "                    xcenter /= 1000\n",
        "                    ycenter /= 1000\n",
        "                    w /= 1000\n",
        "                    h /= 1000\n",
        "                    annotations.append([file_name.replace('.txt', '.tif'), 0 ,xcenter,ycenter, w, h])\n",
        "\n",
        "    # Write the annotations to the CSV file\n",
        "    with open(csv_file, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        #writer.writerow(['filename', 'class_name','x1', 'y1', 'x2', 'y2'])\n",
        "        writer.writerows(annotations)\n",
        "        "
      ],
      "metadata": {
        "id": "_k8Wik8p9qyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the directory path where the text files are located\n",
        "train_truth_path = os.path.join(dataset_path,'TrainGroundTruth')\n",
        "train_image_path = os.path.join(dataset_path,'TrainImages')\n",
        "test_truth_path = os.path.join(dataset_path,'TestGroundTruth')\n",
        "test_image_path = os.path.join(dataset_path,'TestImages')\n",
        "\n",
        "\n",
        "get_data_annotate(train_truth_path, \"train_data.csv\",train_image_path)\n",
        "get_data_annotate(test_truth_path, \"test_data.csv\",test_image_path)"
      ],
      "metadata": {
        "id": "UIiqWG7y-E0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train_data.csv',names=['filename','label','x_center','y_center','width','height'])\n",
        "df.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "O_kbtyJd-4Wz",
        "outputId": "f2303a4b-2511-47b8-c33d-8147e29db111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  filename  label  x_center  y_center  width  height\n",
              "0   34.tif      0     0.694      0.66  0.438    0.08"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d1ff7280-7ddb-4bc0-bfb3-422150c22d3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>label</th>\n",
              "      <th>x_center</th>\n",
              "      <th>y_center</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34.tif</td>\n",
              "      <td>0</td>\n",
              "      <td>0.694</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.438</td>\n",
              "      <td>0.08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1ff7280-7ddb-4bc0-bfb3-422150c22d3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d1ff7280-7ddb-4bc0-bfb3-422150c22d3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d1ff7280-7ddb-4bc0-bfb3-422150c22d3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dest_root_path = '/content/dataset'\n",
        "dest_img_path = os.path.join(dest_root_path,'images')\n",
        "dest_label_path = os.path.join(dest_root_path,'labels')\n",
        "dest_train_img_path = os.path.join(dest_root_path,'images/train')\n",
        "dest_train_label_path = os.path.join(dest_root_path, 'labels/train')\n",
        "\n",
        "try:\n",
        "    os.mkdir(dest_root_path)\n",
        "    os.mkdir(dest_img_path)\n",
        "    os.mkdir(dest_label_path)\n",
        "    os.mkdir(dest_train_img_path)\n",
        "    os.mkdir(dest_train_label_path)\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "uNV1zQte-5mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in df.filename.unique():\n",
        "    row = df[df.filename == filename][['label','x_center','y_center','width','height']]\n",
        "    sh.copy(os.path.join(train_image_path,filename), os.path.join(dest_train_img_path,filename))\n",
        "    np.savetxt(os.path.join(dest_train_label_path,filename.replace('tif','txt')), np.array(row), fmt=[\"%d\", \"%f\", \"%f\", \"%f\", \"%f\"])"
      ],
      "metadata": {
        "id": "QPOPowBD_yyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('test_data.csv',names=['filename','label','x_center','y_center','width','height'])\n",
        "df_test.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "J5NjMyjgA8hN",
        "outputId": "d3b45ff2-a02e-4fd7-f8c1-7c70b51ec27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  filename  label  x_center  y_center  width  height\n",
              "0  764.tif      0     0.668     0.844  0.324   0.066"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2306bf1d-986d-4112-afad-61555385c1e7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>label</th>\n",
              "      <th>x_center</th>\n",
              "      <th>y_center</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>764.tif</td>\n",
              "      <td>0</td>\n",
              "      <td>0.668</td>\n",
              "      <td>0.844</td>\n",
              "      <td>0.324</td>\n",
              "      <td>0.066</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2306bf1d-986d-4112-afad-61555385c1e7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2306bf1d-986d-4112-afad-61555385c1e7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2306bf1d-986d-4112-afad-61555385c1e7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dest_val_img_path = os.path.join(dest_root_path,'images/val')\n",
        "dest_val_label_path = os.path.join(dest_root_path, 'labels/val')\n",
        "try:\n",
        "    os.mkdir(dest_val_img_path)\n",
        "    os.mkdir(dest_val_label_path)\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "QD8oUMqkAGMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in df_test.filename.unique():\n",
        "    row = df_test[df_test.filename == filename][['label','x_center','y_center','width','height']]\n",
        "    sh.copy(os.path.join(test_image_path,filename), os.path.join(dest_val_img_path,filename))\n",
        "    np.savetxt(os.path.join(dest_val_label_path,filename.replace('tif','txt')), np.array(row), fmt=[\"%d\", \"%f\", \"%f\", \"%f\", \"%f\"])"
      ],
      "metadata": {
        "id": "Wkt1LSacAVKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset yaml\n",
        "import yaml\n",
        "\n",
        "# Define the file path\n",
        "file_path = \"dataset/structure.yaml\"\n",
        "\n",
        "# Define the file contents as a dictionary\n",
        "file_contents = {\n",
        "    \"path\": \"../dataset\",\n",
        "    \"train\": \"images/train\",\n",
        "    \"val\": \"images/val\",\n",
        "    \"names\": {\n",
        "        0: \"Signature\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Write the file\n",
        "with open(file_path, \"w\") as f:\n",
        "    yaml.dump(file_contents, f)"
      ],
      "metadata": {
        "id": "R68JNiNKAZhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defult iou = 0.65\n",
        "! python yolov5/train.py --img 600 --batch 16 --epochs 50 --data dataset/structure.yaml --weights yolov5/yolov5x.pt --hyp yolov5/data/hyps/hyp.scratch-low.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vyIYKpbBAXx",
        "outputId": "881fd587-15bc-4c47-c710-aa80461a5aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5/yolov5x.pt, cfg=, data=dataset/structure.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=600, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n",
            "YOLOv5 🚀 v7.0-145-g94714fe Python-3.9.16 torch-2.0.0+cu118 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      8800  models.common.Conv                      [3, 80, 6, 2, 2]              \n",
            "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
            "  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \n",
            "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
            "  4                -1  8   2259200  models.common.C3                        [320, 320, 8]                 \n",
            "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
            "  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \n",
            "  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n",
            "  8                -1  4  19676160  models.common.C3                        [1280, 1280, 4]               \n",
            "  9                -1  1   4099840  models.common.SPPF                      [1280, 1280, 5]               \n",
            " 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n",
            " 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \n",
            " 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \n",
            " 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
            " 24      [17, 20, 23]  1     40374  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n",
            "Model summary: 445 layers, 86217814 parameters, 86217814 gradients\n",
            "\n",
            "Transferred 739/745 items from yolov5/yolov5x.pt\n",
            "WARNING ⚠️ --img-size 600 must be multiple of max stride 32, updating to 608\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 123 weight(decay=0.0), 126 weight(decay=0.0005), 126 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset/labels/train.cache... 659 images, 0 backgrounds, 0 corrupt: 100% 659/659 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/labels/val.cache... 115 images, 0 backgrounds, 0 corrupt: 100% 115/115 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.26 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to yolov5/runs/train/exp2/labels.jpg... \n",
            "Image sizes 608 train, 608 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1myolov5/runs/train/exp2\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "  0% 0/42 [00:00<?, ?it/s]^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python yolov5/val.py --batch 16 --data dataset/structure.yaml --task val --weights yolov5/runs/train/exp/weights/best.pt --name best_val_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjSK7Kj-BEPS",
        "outputId": "f03e02f1-bc24-4846-f239-57bc73389702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=dataset/structure.yaml, weights=['yolov5/runs/train/exp/weights/best.pt'], batch_size=16, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=yolov5/runs/val, name=best_val_, exist_ok=False, half=False, dnn=False\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n",
            "YOLOv5 🚀 v7.0-145-g94714fe Python-3.9.16 torch-2.0.0+cu118 CPU\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/val.py\", line 409, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/val.py\", line 380, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/yolov5/val.py\", line 143, in run\n",
            "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
            "  File \"/content/yolov5/models/common.py\", line 344, in __init__\n",
            "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n",
            "  File \"/content/yolov5/models/experimental.py\", line 79, in attempt_load\n",
            "    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 791, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 271, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 252, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'yolov5/runs/train/exp/weights/best.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = 'yolov5/runs/val/best_val_2'\n",
        "for img_name in os.listdir(DIR):\n",
        "    img = cv2.imread(DIR + '/' + img_name)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot()\n",
        "    ax.imshow(img)\n",
        "    print(img_name)\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "MReivU2fBLkr",
        "outputId": "3f7c87cb-d277-4cc6-cfe4-f4e3b27db69d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0ca942633618>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'yolov5/runs/val/best_val_2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov5/runs/val/best_val_2'"
          ]
        }
      ]
    }
  ]
}